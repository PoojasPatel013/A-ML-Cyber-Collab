{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training with data from: data\n",
      "\n",
      "Training combined CNN model...\n",
      "Loading and combining data from all files...\n",
      "\n",
      "Processing file: data\\cyberattack.csv\n",
      "Found target column: Flow Bytes/s in data\\cyberattack.csv\n",
      "Column values: [4000000.       110091.7431   230769.2308   352941.1765    11741.68297]...\n",
      "\n",
      "Features found in data\\cyberattack.csv:\n",
      "Total features: 79\n",
      "First 5 features: [' Destination Port', ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', 'Total Length of Fwd Packets']\n",
      "Last 5 features: ['Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min', ' Label']\n",
      "Number of unique classes: 91644\n",
      "Label distribution: [2357    1    1 ...    4   13    3]\n",
      "Warning: Large number of classes found (91644). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=91643\n",
      "Unique labels: 91644\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (100000, 78)\n",
      "Combined y shape: (100000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 100000\n",
      "Number of unique classes: 95106\n",
      "Label distribution: [863   1   1 ...   5   3   6]\n",
      "Warning: Large number of classes found (95106). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=95105\n",
      "Unique labels: 95106\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (200000, 78)\n",
      "Combined y shape: (200000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 200000\n",
      "Number of unique classes: 69669\n",
      "Label distribution: [   1    4 7422 ...   13   38    7]\n",
      "Warning: Large number of classes found (69669). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=69668\n",
      "Unique labels: 69669\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (300000, 78)\n",
      "Combined y shape: (300000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 300000\n",
      "Number of unique classes: 12575\n",
      "Label distribution: [  1  11   5 ...   4 171   2]\n",
      "Warning: Large number of classes found (12575). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=12574\n",
      "Unique labels: 12575\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (400000, 78)\n",
      "Combined y shape: (400000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 400000\n",
      "Number of unique classes: 21870\n",
      "Label distribution: [  2  10   1 ...   4 144   7]\n",
      "Warning: Large number of classes found (21870). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=21869\n",
      "Unique labels: 21870\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (500000, 78)\n",
      "Combined y shape: (500000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 500000\n",
      "Number of unique classes: 68017\n",
      "Label distribution: [   4    1 7686 ...    4   69   13]\n",
      "Warning: Large number of classes found (68017). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=68016\n",
      "Unique labels: 68017\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (600000, 78)\n",
      "Combined y shape: (600000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 600000\n",
      "Number of unique classes: 69020\n",
      "Label distribution: [   4 8937    1 ...    4   37   14]\n",
      "Warning: Large number of classes found (69020). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=69019\n",
      "Unique labels: 69020\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (700000, 78)\n",
      "Combined y shape: (700000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 700000\n",
      "Number of unique classes: 79653\n",
      "Label distribution: [   3 6939    1 ...    2   77   17]\n",
      "Warning: Large number of classes found (79653). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=79652\n",
      "Unique labels: 79653\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (800000, 78)\n",
      "Combined y shape: (800000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 800000\n",
      "Number of unique classes: 72382\n",
      "Label distribution: [    2 10722     1 ...     1    75    11]\n",
      "Warning: Large number of classes found (72382). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=72381\n",
      "Unique labels: 72382\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (900000, 78)\n",
      "Combined y shape: (900000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 900000\n",
      "Number of unique classes: 79084\n",
      "Label distribution: [   2 5789    1 ...   25   44    9]\n",
      "Warning: Large number of classes found (79084). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=79083\n",
      "Unique labels: 79084\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1000000, 78)\n",
      "Combined y shape: (1000000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1000000\n",
      "Number of unique classes: 60473\n",
      "Label distribution: [    2 13033     1 ...     3    76    11]\n",
      "Warning: Large number of classes found (60473). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=60472\n",
      "Unique labels: 60473\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1100000, 78)\n",
      "Combined y shape: (1100000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1100000\n",
      "Number of unique classes: 48507\n",
      "Label distribution: [16149     1     1 ...     1    79    15]\n",
      "Warning: Large number of classes found (48507). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=48506\n",
      "Unique labels: 48507\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1200000, 78)\n",
      "Combined y shape: (1200000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1200000\n",
      "Number of unique classes: 64688\n",
      "Label distribution: [    2 11447     1 ...     9    48     4]\n",
      "Warning: Large number of classes found (64688). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=64687\n",
      "Unique labels: 64688\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1300000, 78)\n",
      "Combined y shape: (1300000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1300000\n",
      "Number of unique classes: 47286\n",
      "Label distribution: [   1 9235    1 ...    4   75   11]\n",
      "Warning: Large number of classes found (47286). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=47285\n",
      "Unique labels: 47286\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1400000, 78)\n",
      "Combined y shape: (1400000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1400000\n",
      "Number of unique classes: 46166\n",
      "Label distribution: [6824    1    1 ...    5   84    5]\n",
      "Warning: Large number of classes found (46166). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=46165\n",
      "Unique labels: 46166\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1500000, 78)\n",
      "Combined y shape: (1500000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1500000\n",
      "Number of unique classes: 67444\n",
      "Label distribution: [ 1  5  1 ...  3 75  8]\n",
      "Warning: Large number of classes found (67444). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=67443\n",
      "Unique labels: 67444\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1600000, 78)\n",
      "Combined y shape: (1600000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1600000\n",
      "Number of unique classes: 69010\n",
      "Label distribution: [    4 11098     3 ...    11    50    22]\n",
      "Warning: Large number of classes found (69010). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=69009\n",
      "Unique labels: 69010\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1700000, 78)\n",
      "Combined y shape: (1700000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1700000\n",
      "Number of unique classes: 71683\n",
      "Label distribution: [18260     1     1 ...     4    17    42]\n",
      "Warning: Large number of classes found (71683). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=71682\n",
      "Unique labels: 71683\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1800000, 78)\n",
      "Combined y shape: (1800000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1800000\n",
      "Number of unique classes: 71888\n",
      "Label distribution: [20439     1     1 ...     9    22    37]\n",
      "Warning: Large number of classes found (71888). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=71887\n",
      "Unique labels: 71888\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (1900000, 78)\n",
      "Combined y shape: (1900000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 1900000\n",
      "Number of unique classes: 65150\n",
      "Label distribution: [22701     1     1 ...     1    20    41]\n",
      "Warning: Large number of classes found (65150). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=65149\n",
      "Unique labels: 65150\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2000000, 78)\n",
      "Combined y shape: (2000000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2000000\n",
      "Number of unique classes: 46922\n",
      "Label distribution: [33681     1     1 ...     2     1    54]\n",
      "Warning: Large number of classes found (46922). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=46921\n",
      "Unique labels: 46922\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2100000, 78)\n",
      "Combined y shape: (2100000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2100000\n",
      "Number of unique classes: 67555\n",
      "Label distribution: [    4 18018     1 ...     5    47    27]\n",
      "Warning: Large number of classes found (67555). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=67554\n",
      "Unique labels: 67555\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2200000, 78)\n",
      "Combined y shape: (2200000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2200000\n",
      "Number of unique classes: 76292\n",
      "Label distribution: [22018     1     1 ...     1     7   283]\n",
      "Warning: Large number of classes found (76292). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=76291\n",
      "Unique labels: 76292\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2300000, 78)\n",
      "Combined y shape: (2300000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2300000\n",
      "Number of unique classes: 73703\n",
      "Label distribution: [    1 25388     1 ...     3     2   431]\n",
      "Warning: Large number of classes found (73703). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=73702\n",
      "Unique labels: 73703\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2400000, 78)\n",
      "Combined y shape: (2400000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2400000\n",
      "Number of unique classes: 65831\n",
      "Label distribution: [25179     1     1 ...    19    34   251]\n",
      "Warning: Large number of classes found (65831). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=65830\n",
      "Unique labels: 65831\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2500000, 78)\n",
      "Combined y shape: (2500000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2500000\n",
      "Number of unique classes: 78659\n",
      "Label distribution: [   3 5475    1 ...   16   65   18]\n",
      "Warning: Large number of classes found (78659). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=78658\n",
      "Unique labels: 78659\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2600000, 78)\n",
      "Combined y shape: (2600000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2600000\n",
      "Number of unique classes: 66859\n",
      "Label distribution: [    4 10711     1 ...     9    58     8]\n",
      "Warning: Large number of classes found (66859). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=66858\n",
      "Unique labels: 66859\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2700000, 78)\n",
      "Combined y shape: (2700000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2700000\n",
      "Number of unique classes: 51319\n",
      "Label distribution: [    4 15040     1 ...     1    55     5]\n",
      "Warning: Large number of classes found (51319). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=51318\n",
      "Unique labels: 51319\n",
      "X array shape: (100000, 78)\n",
      "y array shape: (100000,)\n",
      "Combined X shape: (2800000, 78)\n",
      "Combined y shape: (2800000,)\n",
      "Processed chunk with 100000 samples\n",
      "Current memory usage: 31.20 MB\n",
      "Total samples so far: 2800000\n",
      "Number of unique classes: 18678\n",
      "Label distribution: [4108    1    1 ...    1   21    2]\n",
      "Warning: Large number of classes found (18678). This might indicate a problem with the target column.\n",
      "Label value range: min=0, max=18677\n",
      "Unique labels: 18678\n",
      "X array shape: (30743, 78)\n",
      "y array shape: (30743,)\n",
      "Combined X shape: (2830743, 78)\n",
      "Combined y shape: (2830743,)\n",
      "Processed chunk with 30743 samples\n",
      "Current memory usage: 9.59 MB\n",
      "Total samples so far: 2830743\n",
      "\n",
      "Processing file: data\\eight.csv\n",
      "Found target column: Protocol in data\\eight.csv\n",
      "Column values: [17. nan  1.  6.]...\n",
      "\n",
      "Features found in data\\eight.csv:\n",
      "Total features: 5\n",
      "First 5 features: ['Timestamp', 'Source IP', 'Destination IP', 'Protocol', 'Length']\n",
      "Last 5 features: ['Timestamp', 'Source IP', 'Destination IP', 'Protocol', 'Length']\n",
      "Number of unique classes: 4\n",
      "Label distribution: [ 1324 97215  1190   271]\n",
      "Label value range: min=0, max=3\n",
      "Unique labels: 4\n",
      "Warning: Error processing data\\eight.csv: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 78 and the array at index 1 has size 83\n",
      "\n",
      "Processing file: data\\four.csv\n",
      "Found target column: Type in data\\four.csv\n",
      "Column values: [0 1 2 3 4]...\n",
      "\n",
      "Features found in data\\four.csv:\n",
      "Total features: 631\n",
      "First 5 features: ['SHA256', 'Type', 'advapi32.dll', 'kernel32.dll', 'vspmsg.dll']\n",
      "Last 5 features: ['api-ms-win-core-fibers-l1-1-0.dll', 'api-ms-win-core-file-l2-1-0.dll', 'api-ms-win-core-sysinfo-l1-2-0.dll', 'dbgeng.dll', 'd3d11.dll']\n",
      "Number of unique classes: 7\n",
      "Label distribution: [1877 5022 4643 4957 5076 4224 3699]\n",
      "Label value range: min=0, max=6\n",
      "Unique labels: 7\n",
      "Warning: Error processing data\\four.csv: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 78 and the array at index 1 has size 714\n",
      "\n",
      "Processing file: data\\one.csv\n",
      "Found target column: phishing in data\\one.csv\n",
      "Column values: [0 1]...\n",
      "\n",
      "Features found in data\\one.csv:\n",
      "Total features: 20\n",
      "First 5 features: ['url_length', 'n_dots', 'n_hypens', 'n_underline', 'n_slash']\n",
      "Last 5 features: ['n_hastag', 'n_dollar', 'n_percent', 'n_redirection', 'phishing']\n",
      "Number of unique classes: 2\n",
      "Label distribution: [63665 36335]\n",
      "Label value range: min=0, max=1\n",
      "Unique labels: 2\n",
      "Warning: Error processing data\\one.csv: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 78 and the array at index 1 has size 734\n",
      "\n",
      "Processing file: data\\seven.csv\n",
      "Found target column: Protocol in data\\seven.csv\n",
      "Column values: ['NBNS' 'ARP' 'RARP' 'BROWSER' 'ICMPv6']...\n",
      "\n",
      "Features found in data\\seven.csv:\n",
      "Total features: 7\n",
      "First 5 features: ['Time', 'Source', 'No.', 'Destination', 'Protocol']\n",
      "Last 5 features: ['No.', 'Destination', 'Protocol', 'Length', 'Info']\n",
      "Number of unique classes: 15\n",
      "Label distribution: [  189     3     7  1079     1  1324     3    97   188    79     7     4\n",
      " 70021   396 26602]\n",
      "Label value range: min=0, max=14\n",
      "Unique labels: 15\n",
      "Warning: Error processing data\\seven.csv: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 78 and the array at index 1 has size 739\n",
      "\n",
      "Processing file: data\\six.csv\n",
      "Found target column: Prediction in data\\six.csv\n",
      "Column values: ['SS' 'A' 'S']...\n",
      "\n",
      "Features found in data\\six.csv:\n",
      "Total features: 14\n",
      "First 5 features: ['Time', 'Protcol', 'Flag', 'Family', 'Clusters']\n",
      "Last 5 features: ['Netflow_Bytes', 'IPaddress', 'Threats', 'Port', 'Prediction']\n",
      "Number of unique classes: 3\n",
      "Label distribution: [26397 46264 27339]\n",
      "Label value range: min=0, max=2\n",
      "Unique labels: 3\n",
      "Warning: Error processing data\\six.csv: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 78 and the array at index 1 has size 752\n",
      "\n",
      "Processing file: data\\three.csv\n",
      "Found target column: attack in data\\three.csv\n",
      "Column values: [1]...\n",
      "\n",
      "Features found in data\\three.csv:\n",
      "Total features: 47\n",
      "First 5 features: ['Unnamed: 0', 'pkSeqID', 'stime', 'flgs', 'flgs_number']\n",
      "Last 5 features: ['Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP', 'attack', 'category', 'subcategory']\n",
      "Number of unique classes: 1\n",
      "Label distribution: [100000]\n",
      "Label value range: min=0, max=0\n",
      "Unique labels: 1\n",
      "Warning: Error processing data\\three.csv: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 78 and the array at index 1 has size 799\n",
      "\n",
      "Processing file: data\\two.csv\n",
      "Found target column: Attack_type in data\\two.csv\n",
      "Column values: ['MQTT_Publish' 'Thing_Speak' 'Wipro_bulb' 'ARP_poisioning'\n",
      " 'DDOS_Slowloris']...\n",
      "\n",
      "Features found in data\\two.csv:\n",
      "Total features: 85\n",
      "First 5 features: ['Unnamed: 0', 'id.orig_p', 'id.resp_p', 'proto', 'service']\n",
      "Last 5 features: ['idle.std', 'fwd_init_window_size', 'bwd_init_window_size', 'fwd_last_window_size', 'Attack_type']\n",
      "Number of unique classes: 6\n",
      "Label distribution: [ 7750   534 79209  4146  8108   253]\n",
      "Label value range: min=0, max=5\n",
      "Unique labels: 6\n",
      "Warning: Error processing data\\two.csv: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 78 and the array at index 1 has size 882\n",
      "\n",
      "Combined dataset stats:\n",
      "Total samples: 2830743\n",
      "Feature shape: (2830743, 78)\n",
      "Label shape: (2830743,)\n",
      "Unique labels: [    0     1     2 ... 95103 95104 95105]\n",
      "Total memory usage: 883.19 MB\n",
      "\n",
      "Final dataset stats:\n",
      "Total samples: 2830743\n",
      "Number of features: 78\n",
      "Number of classes: 95106\n",
      "Class distribution: [172624 151874  15141 ...      5      3      6]\n",
      "Memory usage: 883.19 MB\n",
      "\n",
      "Starting training with input size: 78, num_classes: 95106\n",
      "Total samples: 2830743\n",
      "Epoch [1/50], Batch [10/88461], Loss: 10.6260\n",
      "Epoch [1/50], Batch [20/88461], Loss: 11.3329\n",
      "Epoch [1/50], Batch [30/88461], Loss: nan\n",
      "Epoch [1/50], Batch [40/88461], Loss: nan\n",
      "Epoch [1/50], Batch [50/88461], Loss: nan\n",
      "Epoch [1/50], Batch [60/88461], Loss: nan\n",
      "Epoch [1/50], Batch [70/88461], Loss: nan\n",
      "Epoch [1/50], Batch [80/88461], Loss: nan\n",
      "Epoch [1/50], Batch [90/88461], Loss: nan\n",
      "Epoch [1/50], Batch [100/88461], Loss: nan\n",
      "Epoch [1/50], Batch [110/88461], Loss: nan\n",
      "Epoch [1/50], Batch [120/88461], Loss: nan\n",
      "Epoch [1/50], Batch [130/88461], Loss: nan\n",
      "Epoch [1/50], Batch [140/88461], Loss: nan\n",
      "Epoch [1/50], Batch [150/88461], Loss: nan\n",
      "Epoch [1/50], Batch [160/88461], Loss: nan\n",
      "Epoch [1/50], Batch [170/88461], Loss: nan\n",
      "Epoch [1/50], Batch [180/88461], Loss: nan\n",
      "Epoch [1/50], Batch [190/88461], Loss: nan\n",
      "Epoch [1/50], Batch [200/88461], Loss: nan\n",
      "Epoch [1/50], Batch [210/88461], Loss: nan\n",
      "Epoch [1/50], Batch [220/88461], Loss: nan\n",
      "Epoch [1/50], Batch [230/88461], Loss: nan\n",
      "Epoch [1/50], Batch [240/88461], Loss: nan\n",
      "Epoch [1/50], Batch [250/88461], Loss: nan\n",
      "Epoch [1/50], Batch [260/88461], Loss: nan\n",
      "Epoch [1/50], Batch [270/88461], Loss: nan\n",
      "Epoch [1/50], Batch [280/88461], Loss: nan\n",
      "Epoch [1/50], Batch [290/88461], Loss: nan\n",
      "Epoch [1/50], Batch [300/88461], Loss: nan\n",
      "Epoch [1/50], Batch [310/88461], Loss: nan\n",
      "Epoch [1/50], Batch [320/88461], Loss: nan\n",
      "Epoch [1/50], Batch [330/88461], Loss: nan\n",
      "Epoch [1/50], Batch [340/88461], Loss: nan\n",
      "Epoch [1/50], Batch [350/88461], Loss: nan\n",
      "Epoch [1/50], Batch [360/88461], Loss: nan\n",
      "Epoch [1/50], Batch [370/88461], Loss: nan\n",
      "Epoch [1/50], Batch [380/88461], Loss: nan\n",
      "Epoch [1/50], Batch [390/88461], Loss: nan\n",
      "Epoch [1/50], Batch [400/88461], Loss: nan\n",
      "Epoch [1/50], Batch [410/88461], Loss: nan\n",
      "Epoch [1/50], Batch [420/88461], Loss: nan\n",
      "Epoch [1/50], Batch [430/88461], Loss: nan\n",
      "Epoch [1/50], Batch [440/88461], Loss: nan\n",
      "Epoch [1/50], Batch [450/88461], Loss: nan\n",
      "Epoch [1/50], Batch [460/88461], Loss: nan\n",
      "Epoch [1/50], Batch [470/88461], Loss: nan\n",
      "Epoch [1/50], Batch [480/88461], Loss: nan\n",
      "Epoch [1/50], Batch [490/88461], Loss: nan\n",
      "Epoch [1/50], Batch [500/88461], Loss: nan\n",
      "Epoch [1/50], Batch [510/88461], Loss: nan\n",
      "Epoch [1/50], Batch [520/88461], Loss: nan\n",
      "Epoch [1/50], Batch [530/88461], Loss: nan\n",
      "Epoch [1/50], Batch [540/88461], Loss: nan\n",
      "Epoch [1/50], Batch [550/88461], Loss: nan\n",
      "Epoch [1/50], Batch [560/88461], Loss: nan\n",
      "Epoch [1/50], Batch [570/88461], Loss: nan\n",
      "Epoch [1/50], Batch [580/88461], Loss: nan\n",
      "Epoch [1/50], Batch [590/88461], Loss: nan\n",
      "Epoch [1/50], Batch [600/88461], Loss: nan\n",
      "Epoch [1/50], Batch [610/88461], Loss: nan\n",
      "Epoch [1/50], Batch [620/88461], Loss: nan\n",
      "Epoch [1/50], Batch [630/88461], Loss: nan\n",
      "Epoch [1/50], Batch [640/88461], Loss: nan\n",
      "Epoch [1/50], Batch [650/88461], Loss: nan\n",
      "Epoch [1/50], Batch [660/88461], Loss: nan\n",
      "Epoch [1/50], Batch [670/88461], Loss: nan\n",
      "Epoch [1/50], Batch [680/88461], Loss: nan\n",
      "Epoch [1/50], Batch [690/88461], Loss: nan\n",
      "Epoch [1/50], Batch [700/88461], Loss: nan\n",
      "Epoch [1/50], Batch [710/88461], Loss: nan\n",
      "Epoch [1/50], Batch [720/88461], Loss: nan\n",
      "Epoch [1/50], Batch [730/88461], Loss: nan\n",
      "Epoch [1/50], Batch [740/88461], Loss: nan\n",
      "Epoch [1/50], Batch [750/88461], Loss: nan\n",
      "Epoch [1/50], Batch [760/88461], Loss: nan\n",
      "Epoch [1/50], Batch [770/88461], Loss: nan\n",
      "Epoch [1/50], Batch [780/88461], Loss: nan\n",
      "Epoch [1/50], Batch [790/88461], Loss: nan\n",
      "Epoch [1/50], Batch [800/88461], Loss: nan\n",
      "Epoch [1/50], Batch [810/88461], Loss: nan\n",
      "Epoch [1/50], Batch [820/88461], Loss: nan\n",
      "Epoch [1/50], Batch [830/88461], Loss: nan\n",
      "Epoch [1/50], Batch [840/88461], Loss: nan\n",
      "Epoch [1/50], Batch [850/88461], Loss: nan\n",
      "Epoch [1/50], Batch [860/88461], Loss: nan\n",
      "Epoch [1/50], Batch [870/88461], Loss: nan\n",
      "Epoch [1/50], Batch [880/88461], Loss: nan\n",
      "Epoch [1/50], Batch [890/88461], Loss: nan\n",
      "Epoch [1/50], Batch [900/88461], Loss: nan\n",
      "Epoch [1/50], Batch [910/88461], Loss: nan\n",
      "Epoch [1/50], Batch [920/88461], Loss: nan\n",
      "Epoch [1/50], Batch [930/88461], Loss: nan\n",
      "Epoch [1/50], Batch [940/88461], Loss: nan\n",
      "Epoch [1/50], Batch [950/88461], Loss: nan\n",
      "Epoch [1/50], Batch [960/88461], Loss: nan\n",
      "Epoch [1/50], Batch [970/88461], Loss: nan\n",
      "Epoch [1/50], Batch [980/88461], Loss: nan\n",
      "Epoch [1/50], Batch [990/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1000/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1010/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1020/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1030/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1040/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1050/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1060/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1070/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1080/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1090/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1100/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1110/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1120/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1130/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1140/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1150/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1160/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1170/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1180/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1190/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1200/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1210/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1220/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1230/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1240/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1250/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1260/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1270/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1280/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1290/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1300/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1310/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1320/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1330/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1340/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1350/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1360/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1370/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1380/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1390/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1400/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1410/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1420/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1430/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1440/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1450/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1460/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1470/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1480/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1490/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1500/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1510/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1520/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1530/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1540/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1550/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1560/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1570/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1580/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1590/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1600/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1610/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1620/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1630/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1640/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1650/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1660/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1670/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1680/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1690/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1700/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1710/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1720/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1730/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1740/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1750/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1760/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1770/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1780/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1790/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1800/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1810/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1820/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1830/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1840/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1850/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1860/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1870/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1880/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1890/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1900/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1910/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1920/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1930/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1940/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1950/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1960/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1970/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1980/88461], Loss: nan\n",
      "Epoch [1/50], Batch [1990/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2000/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2010/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2020/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2030/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2040/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2050/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2060/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2070/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2080/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2090/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2100/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2110/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2120/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2130/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2140/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2150/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2160/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2170/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2180/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2190/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2200/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2210/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2220/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2230/88461], Loss: nan\n",
      "Epoch [1/50], Batch [2240/88461], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CombinedCyberThreatDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.num_classes = None\n",
    "        self._load_and_combine_data()\n",
    "        \n",
    "        # Validate the combined data\n",
    "        if self.X is not None and self.y is not None:\n",
    "            assert len(self.X) == len(self.y), \"X and y must have same length\"\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            print(f\"\\nFinal dataset stats:\")\n",
    "            print(f\"Total samples: {len(self.X)}\")\n",
    "            print(f\"Number of features: {self.X.shape[1]}\")\n",
    "            print(f\"Number of classes: {self.num_classes}\")\n",
    "            print(f\"Class distribution: {np.bincount(self.y)}\")\n",
    "            print(f\"Memory usage: {self.X.nbytes / 1e6:.2f} MB\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid data loaded from files\")\n",
    "            \n",
    "    def get_num_classes(self):\n",
    "        \"\"\"Get the number of unique classes in the dataset\"\"\"\n",
    "        return self.num_classes if self.num_classes is not None else 0\n",
    "        \n",
    "    def _load_and_combine_data(self):\n",
    "        \"\"\"Load and combine data from files efficiently\"\"\"\n",
    "        print(\"Loading and combining data from all files...\")\n",
    "        \n",
    "        # First pass: Collect all unique features\n",
    "        all_features = []\n",
    "        chunk_size = 100000  # Process in chunks to handle large files\n",
    "        \n",
    "        # Process each file and combine immediately\n",
    "        for current_file in self.all_files:\n",
    "            print(f\"\\nProcessing file: {current_file}\")\n",
    "            try:\n",
    "                # Process file in chunks\n",
    "                first_chunk = True\n",
    "                for chunk in pd.read_csv(current_file, chunksize=chunk_size):\n",
    "                    if first_chunk:\n",
    "                        # Detect target column\n",
    "                        target_col = None\n",
    "                        patterns = [\n",
    "                            'label', 'prediction', 'type', 'phishing', 'attack_type', 'attack',\n",
    "                            'class', 'result', 'target', 'y', 'outcome', 'category'\n",
    "                        ]\n",
    "                        \n",
    "                        # Convert all column names to lowercase\n",
    "                        lower_cols = [col.lower() for col in chunk.columns]\n",
    "                        \n",
    "                        # Try exact matches first\n",
    "                        for pattern in patterns:\n",
    "                            if pattern in lower_cols:\n",
    "                                target_col = chunk.columns[lower_cols.index(pattern)]\n",
    "                                break\n",
    "                        \n",
    "                        # If not found, try partial matches\n",
    "                        if not target_col:\n",
    "                            for col, lower_col in zip(chunk.columns, lower_cols):\n",
    "                                if any(pattern in lower_col for pattern in patterns):\n",
    "                                    target_col = col\n",
    "                                    break\n",
    "                        \n",
    "                        # If still not found, try to find the column with the least unique values\n",
    "                        if not target_col:\n",
    "                            min_unique = float('inf')\n",
    "                            for col in chunk.columns:\n",
    "                                try:\n",
    "                                    unique_count = chunk[col].nunique()\n",
    "                                    if unique_count < min_unique:\n",
    "                                        min_unique = unique_count\n",
    "                                        target_col = col\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Warning: Error checking unique values for {col}: {e}\")\n",
    "                        \n",
    "                        if not target_col:\n",
    "                            print(f\"Warning: No target column found in {current_file}\")\n",
    "                            break\n",
    "                        \n",
    "                        print(f\"Found target column: {target_col} in {current_file}\")\n",
    "                        print(f\"Column values: {chunk[target_col].unique()[:5]}...\")\n",
    "                        \n",
    "                        # Add features from this file\n",
    "                        if not all_features:  # Initialize with first file's features\n",
    "                            all_features = chunk.columns.tolist()\n",
    "                        else:\n",
    "                            # Add new features from this file\n",
    "                            new_features = set(chunk.columns) - set(all_features)\n",
    "                            all_features.extend(sorted(new_features))\n",
    "                        \n",
    "                        first_chunk = False\n",
    "                        \n",
    "                        # Print feature stats\n",
    "                        print(f\"\\nFeatures found in {current_file}:\")\n",
    "                        print(f\"Total features: {len(chunk.columns)}\")\n",
    "                        print(f\"First 5 features: {chunk.columns[:5].tolist()}\")\n",
    "                        print(f\"Last 5 features: {chunk.columns[-5:].tolist()}\")\n",
    "                        \n",
    "                    # Create aligned DataFrame with all features\n",
    "                    df_aligned = pd.DataFrame(columns=all_features)\n",
    "                    \n",
    "                    # Fill with data where available, 0 where not\n",
    "                    for col in all_features:\n",
    "                        if col in chunk.columns:\n",
    "                            df_aligned[col] = chunk[col]\n",
    "                        else:\n",
    "                            df_aligned[col] = 0\n",
    "                    \n",
    "                    # Process features\n",
    "                    X = df_aligned.drop(target_col, axis=1)\n",
    "                    y = chunk[target_col]\n",
    "                    \n",
    "                    # Handle categorical features\n",
    "                    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "                    for col in categorical_cols:\n",
    "                        X[col] = pd.factorize(X[col])[0]\n",
    "                    \n",
    "                    # Convert to numeric and use float32\n",
    "                    X = X.astype('float32').fillna(0)\n",
    "                    \n",
    "                    # Convert labels using LabelEncoder\n",
    "                    le = LabelEncoder()\n",
    "                    y = le.fit_transform(y)\n",
    "                    \n",
    "                    # Validate feature shape\n",
    "                    if X.shape[1] != len(all_features) - 1:\n",
    "                        raise ValueError(f\"Feature shape mismatch in {current_file}: Expected {len(all_features) - 1}, got {X.shape[1]}\")\n",
    "                    \n",
    "                    # Validate label values\n",
    "                    if y.min() < 0:  # Labels should always be non-negative\n",
    "                        raise ValueError(f\"Negative label values found: min={y.min()}\")\n",
    "                    \n",
    "                    # Print label statistics\n",
    "                    num_classes = len(np.unique(y))\n",
    "                    print(f\"Number of unique classes: {num_classes}\")\n",
    "                    print(f\"Label distribution: {np.bincount(y)}\")\n",
    "                    \n",
    "                    if num_classes > 1000:  # If too many classes, raise warning\n",
    "                        print(f\"Warning: Large number of classes found ({num_classes}). This might indicate a problem with the target column.\")\n",
    "                    \n",
    "                    # Convert to numpy arrays immediately\n",
    "                    X_array = X.to_numpy(dtype=np.float32)\n",
    "                    y_array = np.array(y, dtype=np.int32)\n",
    "                    \n",
    "                    # Print label statistics\n",
    "                    print(f\"Label value range: min={y.min()}, max={y.max()}\")\n",
    "                    print(f\"Unique labels: {len(np.unique(y))}\")\n",
    "                    \n",
    "                    # If this is the first chunk, initialize arrays\n",
    "                    if self.X is None:\n",
    "                        self.X = X_array\n",
    "                        self.y = y_array\n",
    "                    else:\n",
    "                        # Append to existing arrays\n",
    "                        self.X = np.vstack([self.X, X_array])\n",
    "                        self.y = np.concatenate([self.y, y_array])\n",
    "                        \n",
    "                    # Print array shapes for debugging\n",
    "                    print(f\"X array shape: {X_array.shape}\")\n",
    "                    print(f\"y array shape: {y_array.shape}\")\n",
    "                    print(f\"Combined X shape: {self.X.shape if self.X is not None else 'None'}\")\n",
    "                    print(f\"Combined y shape: {self.y.shape if self.y is not None else 'None'}\")\n",
    "                    \n",
    "                    print(f\"Processed chunk with {len(X_array)} samples\")\n",
    "                    print(f\"Current memory usage: {X_array.nbytes / 1e6:.2f} MB\")\n",
    "                    print(f\"Total samples so far: {len(self.X)}\")\n",
    "                    \n",
    "                    # Clean up\n",
    "                    del X, y, X_array, y_array, df_aligned\n",
    "                    gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing {current_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if self.X is not None:\n",
    "            print(f\"\\nCombined dataset stats:\")\n",
    "            print(f\"Total samples: {len(self.X)}\")\n",
    "            print(f\"Feature shape: {self.X.shape}\")\n",
    "            print(f\"Label shape: {self.y.shape}\")\n",
    "            print(f\"Unique labels: {np.unique(self.y)}\")\n",
    "            print(f\"Total memory usage: {self.X.nbytes / 1e6:.2f} MB\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid data loaded from files\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            X = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "            \n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item: {e}\")\n",
    "            raise\n",
    "\n",
    "class CombinedCyberThreatCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CombinedCyberThreatCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(1, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Calculate output size\n",
    "        self.output_size = self._get_output_size(input_size)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.output_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def _get_output_size(self, input_size):\n",
    "        x = torch.randn(1, 1, input_size)\n",
    "        x = self.feature_extractor(x)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_combined_model(data_dir):\n",
    "    try:\n",
    "        print(\"\\nTraining combined CNN model...\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = CombinedCyberThreatDataset(data_dir)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Get input size and number of classes from dataset\n",
    "        input_size = dataset.X.shape[1]\n",
    "        num_classes = dataset.get_num_classes()\n",
    "        \n",
    "        # Initialize model, loss, and optimizer\n",
    "        model = CombinedCyberThreatCNN(input_size, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Create save directory\n",
    "        save_dir = 'models_combined'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Training loop\n",
    "        num_epochs = 50\n",
    "        best_accuracy = 0\n",
    "        best_epoch = 0\n",
    "        patience = 10  # Early stopping patience\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        print(f\"\\nStarting training with input size: {input_size}, num_classes: {num_classes}\")\n",
    "        print(f\"Total samples: {len(dataset)}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for i, (X, y) in enumerate(dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "                \n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            train_acc = 100 * correct / total\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            \n",
    "            # Save best model\n",
    "            if train_acc > best_accuracy:\n",
    "                best_accuracy = train_acc\n",
    "                best_epoch = epoch\n",
    "                no_improvement_count = 0\n",
    "                \n",
    "                # Save the model\n",
    "                model_file = os.path.join(save_dir, 'combined_cnn_model.pth')\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'accuracy': train_acc\n",
    "                }, model_file)\n",
    "                print(f\"Saved best model with accuracy: {train_acc:.2f}%\")\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                \n",
    "            # Early stopping\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} after {patience} epochs without improvement\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nTraining complete!\")\n",
    "        print(f\"Best accuracy: {best_accuracy:.2f}% at epoch {best_epoch + 1}\")\n",
    "        print(f\"Model saved to: {model_file}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training combined model: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Create necessary directories\n",
    "        os.makedirs('models_combined', exist_ok=True)\n",
    "        \n",
    "        # Train combined model\n",
    "        data_dir = 'data'\n",
    "        print(f\"\\nStarting training with data from: {data_dir}\")\n",
    "        model = train_combined_model(data_dir)\n",
    "        \n",
    "        if model:\n",
    "            print(\"\\nSuccessfully trained combined CNN model\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
